{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "import libcudnn, ctypes\n",
    "import pycuda.driver as drv\n",
    "import pandas as pd\n",
    "from pycuda.compiler import SourceModule\n",
    "from skcuda import cublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some globals. Should be defined in a constructor for a sequential class.\n",
    "\n",
    "# Initialize the cuDNN context\n",
    "cudnn_context = libcudnn.cudnnCreate()\n",
    "cublas_handle = cublas.cublasCreate()\n",
    "\n",
    "# Set some options and tensor dimensions\n",
    "softmax_mode = libcudnn.cudnnSoftmaxMode['CUDNN_SOFTMAX_MODE_INSTANCE']\n",
    "softmax_algo = libcudnn.cudnnSoftmaxAlgorithm['CUDNN_SOFTMAX_ACCURATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions. !!!! Are these all used more than once? !!!!\n",
    "def create_4d_tensor_desc(tensor_format, data_type, n, c, h, w):\n",
    "    \n",
    "    tensor_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "    libcudnn.cudnnSetTensor4dDescriptor(tensor_desc, tensor_format, data_type, n, c, h, w)\n",
    "    return tensor_desc\n",
    "\n",
    "def create_pooling_2d_desc(mode, propogation_mode, windowHeight, windowWidth, verticalPadding, \n",
    "                           horizontalPadding, verticalStride, horizontalStride):\n",
    "    pooling_desc = libcudnn.cudnnCreatePoolingDescriptor()\n",
    "    libcudnn.cudnnSetPooling2dDescriptor(pooling_desc, mode, propogation_mode,\n",
    "                                         windowHeight, windowWidth, \n",
    "                                         verticalPadding, horizontalPadding, \n",
    "                                         verticalStride, horizontalStride)\n",
    "    return pooling_desc\n",
    "\n",
    "def create_activation_desc(mode, reluNanOpt, coef):\n",
    "    \n",
    "    activation_desc = libcudnn.cudnnCreateActivationDescriptor()\n",
    "    libcudnn.cudnnSetActivationDescriptor(activation_desc, mode, reluNanOpt, coef)\n",
    "    return activation_desc\n",
    "\n",
    "def create_convolution_2d_desc(pad_h, pad_w, u, v, dilation_h, dilation_w, mode, computeType):\n",
    "    \n",
    "    conv2d_desc = libcudnn.cudnnCreateConvolutionDescriptor()\n",
    "    libcudnn.cudnnSetConvolution2dDescriptor(conv2d_desc, pad_h, pad_w, u, v, \n",
    "                                             dilation_h, dilation_w, mode, computeType)\n",
    "    return conv2d_desc\n",
    "\n",
    "def create_filter_4d_desc(data_type, tensor_format, k, c, h, w):\n",
    "    \n",
    "    filter4d_desc = libcudnn.cudnnCreateFilterDescriptor()\n",
    "    libcudnn.cudnnSetFilter4dDescriptor(filter4d_desc, data_type, tensor_format, k, c, h, w)\n",
    "    return filter4d_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_layer:\n",
    "    \n",
    "    def __init__(self, cudnn_context, batch_size, in_channels, in_height, in_width,\n",
    "                 out_channels, kernel_size):\n",
    "        \n",
    "        # !!!! Are these needed? !!!!\n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.tensor_format = libcudnn.cudnnTensorFormat['CUDNN_TENSOR_NCHW']\n",
    "        self.data_type = libcudnn.cudnnDataType['CUDNN_DATA_FLOAT']\n",
    "        self.data_type_np = np.float32\n",
    "        self.pooling_format = libcudnn.cudnnPoolingMode['CUDNN_POOLING_MAX']\n",
    "        self.activation_mode = libcudnn.cudnnActivationMode['CUDNN_ACTIVATION_RELU']\n",
    "        self.propogation_mode = libcudnn.cudnnNanPropagation['CUDNN_PROPAGATE_NAN']\n",
    "        self.convolution_mode = libcudnn.cudnnConvolutionMode['CUDNN_CROSS_CORRELATION']\n",
    "        # !!!! The convolution is found by function calls in the LeNet code. Is choosing this ok? !!!!\n",
    "        self.convolution_algo = libcudnn.cudnnConvolutionFwdAlgo['CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM']\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Store inputs (!!!! note: assumes stride 1, 0 padding !!!!)\n",
    "        self.in_channels = in_channels\n",
    "        self.n_filters = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        #self.out_width = in_width - kernel_size + 1\n",
    "        #self.out_height = in_height - kernel_size + 1\n",
    "        \n",
    "        # Reserve memory\n",
    "        w = np.ones((self.in_channels, \n",
    "                      self.kernel_size, \n",
    "                      self.kernel_size, \n",
    "                      self.n_filters), dtype=self.data_type_np)\n",
    "        w_bias = np.ones((self.n_filters, ), dtype=self.data_type_np)\n",
    "        \n",
    "        # Send memory to GPU\n",
    "        self.w_gpu = gpuarray.to_gpu(w)\n",
    "        self.w_bias_gpu = gpuarray.to_gpu(w_bias)\n",
    "        \n",
    "        # Descriptors\n",
    "        self.w_bias_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, 1, self.n_filters, 1, 1)\n",
    "        self.input_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, self.batch_size, \n",
    "            self.in_channels, self.in_height, self.in_width)\n",
    "        self.w_desc = create_filter_4d_desc(\n",
    "            self.data_type, self.tensor_format, self.n_filters, self.in_channels, \n",
    "            self.kernel_size, self.kernel_size)\n",
    "        self.conv_desc = create_convolution_2d_desc(\n",
    "            0, 0, 1, 1, 1, 1, self.convolution_mode, self.data_type)\n",
    "        self.activation_desc = create_activation_desc(\n",
    "            self.activation_mode, self.propogation_mode, 0.0)  # !!!! What does coef do? !!!!\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        [_, _, self.out_height, self.out_width] = \\\n",
    "        libcudnn.cudnnGetConvolution2dForwardOutputDim(\n",
    "            self.conv_desc, self.input_desc, self.w_desc)\n",
    "        \n",
    "        # Set the output descriptor and allocate space\n",
    "        self.output_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, self.batch_size, \n",
    "            self.n_filters, self.out_height, self.out_width)\n",
    "        self.output_gpu = gpuarray.empty(\n",
    "            (self.batch_size, self.n_filters, self.out_height, self.out_width), \n",
    "            self.data_type_np)\n",
    "        self.ws_size = libcudnn.cudnnGetConvolutionForwardWorkspaceSize(\n",
    "            self.cudnn_context, self.input_desc, self.w_desc,\n",
    "            self.conv_desc, self.output_desc, self.convolution_algo)\n",
    "        \n",
    "        # !!!! Reserve workspace !!!!\n",
    "        if(self.ws_size > 0):\n",
    "                print(f\"Warning: workspace is not 0: {self.ws_size}\")\n",
    "        self.workspace = None\n",
    "    \n",
    "    \n",
    "    def forward(self, input_gpu):\n",
    "        \n",
    "        alpha = 1.0\n",
    "        beta = 0.0\n",
    "\n",
    "        libcudnn.cudnnConvolutionForward(self.cudnn_context, \n",
    "                                         alpha,\n",
    "                                         self.input_desc, input_gpu.ptr, \n",
    "                                         self.w_desc, self.w_gpu.ptr,\n",
    "                                         self.conv_desc, \n",
    "                                         self.convolution_algo,\n",
    "                                         self.workspace, \n",
    "                                         self.ws_size,\n",
    "                                         beta,\n",
    "                                         self.output_desc,\n",
    "                                         self.output_gpu.ptr)\n",
    "        libcudnn.cudnnAddTensor(self.cudnn_context, \n",
    "                                alpha, \n",
    "                                self.w_bias_desc,\n",
    "                                self.w_bias_gpu.ptr,\n",
    "                                alpha,\n",
    "                                self.output_desc,\n",
    "                                self.output_gpu.ptr)\n",
    "        libcudnn.cudnnActivationForward(self.cudnn_context, \n",
    "                                        self.activation_desc, \n",
    "                                        alpha, \n",
    "                                        self.output_desc,\n",
    "                                        self.output_gpu.ptr,\n",
    "                                        beta,\n",
    "                                        self.output_desc,\n",
    "                                        self.output_gpu.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling_layer:\n",
    "    \n",
    "    def __init__(self, cudnn_context, batch_size, in_channels, in_width, in_height,\n",
    "                 stride, kernel_size):\n",
    "        \n",
    "        # !!!! Are these needed? !!!!\n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.tensor_format = libcudnn.cudnnTensorFormat['CUDNN_TENSOR_NCHW']\n",
    "        self.data_type = libcudnn.cudnnDataType['CUDNN_DATA_FLOAT']\n",
    "        self.data_type_np = np.float32\n",
    "        self.pooling_format = libcudnn.cudnnPoolingMode['CUDNN_POOLING_MAX']\n",
    "        self.activation_mode = libcudnn.cudnnActivationMode['CUDNN_ACTIVATION_RELU']\n",
    "        self.propogation_mode = libcudnn.cudnnNanPropagation['CUDNN_PROPAGATE_NAN']\n",
    "        self.convolution_mode = libcudnn.cudnnConvolutionMode['CUDNN_CROSS_CORRELATION']\n",
    "        # !!!! The convolution is found by function calls in the LeNet code. Is choosing this ok? !!!!\n",
    "        self.convolution_algo = libcudnn.cudnnConvolutionFwdAlgo['CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM']\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Store inputs (!!!! assumes 0 padding !!!!)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels  # !!!! out or output? !!!!\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        \n",
    "        # Calculate the output sizes (!!!! Use cudnnGetPoolingNdForwardOutputDim !!!!)\n",
    "        self.out_height = (self.in_height - self.kernel_size) // self.stride + 1\n",
    "        self.out_width = (self.in_width - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # Descriptors\n",
    "        self.input_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, self.batch_size, \n",
    "            self.in_channels, self.in_height, self.in_width)\n",
    "        self.output_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, self.batch_size, \n",
    "            self.out_channels, self.out_height, self.out_width)\n",
    "        self.pooling_desc = create_pooling_2d_desc(\n",
    "            self.pooling_format, self.propogation_mode, \n",
    "            self.kernel_size, self.kernel_size, 0, 0, self.stride, self.stride)\n",
    "        \n",
    "        # Reserve space for the output\n",
    "        self.output_gpu = gpuarray.empty(\n",
    "            (self.batch_size, self.out_channels, self.out_height, self.out_width), \n",
    "            self.data_type_np)\n",
    "    \n",
    "    def forward(self, input_gpu):\n",
    "        \n",
    "        alpha = 1.0\n",
    "        beta = 0.0\n",
    "        \n",
    "        libcudnn.cudnnPoolingForward(self.cudnn_context,\n",
    "                                     self.pooling_desc,\n",
    "                                     alpha,\n",
    "                                     self.input_desc,\n",
    "                                     input_gpu.ptr,\n",
    "                                     beta,\n",
    "                                     self.output_desc,\n",
    "                                     self.output_gpu.ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer:\n",
    "    \n",
    "    def __init__(self, cudnn_context, cublas_handle, batch_size, in_size, \n",
    "                 out_size, activation_type='CUDNN_ACTIVATION_RELU'):\n",
    "        \n",
    "        # !!!! Are these needed? !!!!\n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.tensor_format = libcudnn.cudnnTensorFormat['CUDNN_TENSOR_NCHW']\n",
    "        self.data_type = libcudnn.cudnnDataType['CUDNN_DATA_FLOAT']\n",
    "        self.data_type_np = np.float32\n",
    "        self.pooling_format = libcudnn.cudnnPoolingMode['CUDNN_POOLING_MAX']\n",
    "        self.activation_mode = libcudnn.cudnnActivationMode[activation_type]\n",
    "        self.propogation_mode = libcudnn.cudnnNanPropagation['CUDNN_PROPAGATE_NAN']\n",
    "        self.convolution_mode = libcudnn.cudnnConvolutionMode['CUDNN_CROSS_CORRELATION']\n",
    "        # !!!! The convolution is found by function calls in the LeNet code. Is choosing this ok? !!!!\n",
    "        self.convolution_algo = libcudnn.cudnnConvolutionFwdAlgo['CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM']\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.cublas_handle = cublas_handle\n",
    "        \n",
    "        # Store inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        # Descriptors\n",
    "        self.activation_desc = create_activation_desc(\n",
    "            self.activation_mode, self.propogation_mode, 0.0)  # !!!! What does coef do? !!!!\n",
    "        self.out_desc = create_4d_tensor_desc(\n",
    "            self.tensor_format, self.data_type, self.batch_size, \n",
    "            self.out_size, 1, 1,)\n",
    "        \n",
    "        \n",
    "        # Reserve memory\n",
    "        w = np.ones((self.out_size, self.in_size), dtype=self.data_type_np)\n",
    "        b = np.ones((self.out_size, 1), dtype=self.data_type_np)\n",
    "        self.w_gpu = gpuarray.to_gpu(w)\n",
    "        self.b_gpu = gpuarray.to_gpu(b)\n",
    "        out = np.zeros((self.out_size, self.batch_size), dtype=self.data_type_np)\n",
    "        self.out_gpu = gpuarray.to_gpu(out.T.copy())\n",
    "        #ones = np.ones((1, self.out_size), dtype=np.float32)\n",
    "        ones = np.ones((1, self.batch_size), dtype=np.float32)\n",
    "        self.ones_gpu = gpuarray.to_gpu(ones)\n",
    "        \n",
    "    def forward(self, input_gpu):\n",
    "        \n",
    "        alpha = 1.0\n",
    "        beta = 0.0\n",
    "                \n",
    "        m = self.out_size\n",
    "        k = self.in_size\n",
    "        n = self.batch_size\n",
    "        \n",
    "        # The array may need to be \"flattened\" if the previous layer was a pooling\n",
    "        input_reshaped_gpu = input_gpu.reshape((k, n), order=\"F\")\n",
    "        \n",
    "        print(\"w\")\n",
    "        print(self.w_gpu)\n",
    "        print(\"input\")\n",
    "        print(input_reshaped_gpu)\n",
    "        \n",
    "        \n",
    "        # Weights times inputs\n",
    "        cublas.cublasSgemm(self.cublas_handle, \n",
    "                           cublas._CUBLAS_OP['T'],\n",
    "                           cublas._CUBLAS_OP['N'],\n",
    "                           m, n, k,\n",
    "                           alpha,\n",
    "                           self.w_gpu.gpudata, k,\n",
    "                           input_reshaped_gpu.gpudata, k,\n",
    "                           beta,\n",
    "                           self.out_gpu.gpudata, m)\n",
    "        \n",
    "        print(\"output\")\n",
    "        print(self.out_gpu)\n",
    "        \n",
    "        print(\"b\")\n",
    "        print(self.b_gpu)\n",
    "        \n",
    "        # Add the bias term\n",
    "        cublas.cublasSgemm(self.cublas_handle, \n",
    "                           cublas._CUBLAS_OP['T'],\n",
    "                           cublas._CUBLAS_OP['T'],\n",
    "                           m, n, 1,\n",
    "                           alpha,\n",
    "                           self.b_gpu.gpudata, 1,\n",
    "                           self.ones_gpu.gpudata, n,\n",
    "                           alpha,\n",
    "                           self.out_gpu.gpudata, m)\n",
    "\n",
    "        print(\"output, biased\")\n",
    "        print(self.out_gpu)\n",
    "        \n",
    "        # Apply the activation\n",
    "        libcudnn.cudnnActivationForward(self.cudnn_context, \n",
    "                                        self.activation_desc, \n",
    "                                        alpha, \n",
    "                                        self.out_desc,\n",
    "                                        self.out_gpu.ptr,\n",
    "                                        beta,\n",
    "                                        self.out_desc,\n",
    "                                        self.out_gpu.ptr)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = convolution_layer(cudnn_context=cudnn_context,\n",
    "                               batch_size=2,\n",
    "                               in_channels=1,\n",
    "                               in_height=4,\n",
    "                               in_width=4,\n",
    "                               out_channels=1,\n",
    "                               kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((conv_layer.ws_size, conv_layer.out_height, conv_layer.out_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.ones((2, 1, 4, 4), dtype=np.float32)\n",
    "#input_data[0][0][0][0] = -7\n",
    "\n",
    "input_data_gpu = gpuarray.to_gpu(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer.forward(input_data_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer.output_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer = pooling_layer(cudnn_context=cudnn_context, \n",
    "                              batch_size=2,\n",
    "                              in_channels=1,\n",
    "                              in_width=4,\n",
    "                              in_height=4,\n",
    "                              stride=1,\n",
    "                              kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pool_layer.out_height, pool_layer.out_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.ones((2, 1, 4, 4), dtype=np.float32)\n",
    "input_data[0][0][0][0] = 1\n",
    "input_data[0][0][0][3] = 2\n",
    "input_data[0][0][3][0] = 3\n",
    "input_data[0][0][3][3] = 4\n",
    "input_data_gpu = gpuarray.to_gpu(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer.forward(input_data_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer.output_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_l = dense_layer(cudnn_context=cudnn_context,\n",
    "                      cublas_handle=cublas_handle,\n",
    "                      batch_size=2,\n",
    "                      in_size=4, \n",
    "                      out_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_l.forward(pool_layer.output_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_l.out_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_l2 = dense_layer(cudnn_context=cudnn_context,\n",
    "                       cublas_handle=cublas_handle,\n",
    "                       batch_size=2,\n",
    "                       in_size=3, \n",
    "                       out_size=1)\n",
    "dense_l2.forward(dense_l.out_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
