{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "import libcudnn, ctypes\n",
    "import pycuda.driver as drv\n",
    "import pandas as pd\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some globals. Should be defined in a constructor for a sequential class.\n",
    "\n",
    "# Initialize the cuDNN context\n",
    "cudnn_context = libcudnn.cudnnCreate()\n",
    "\n",
    "# Set some options and tensor dimensions\n",
    "softmax_mode = libcudnn.cudnnSoftmaxMode['CUDNN_SOFTMAX_MODE_INSTANCE']\n",
    "softmax_algo = libcudnn.cudnnSoftmaxAlgorithm['CUDNN_SOFTMAX_ACCURATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_4d_tensor_desc(tensor_format, data_type, n, c, h, w):\n",
    "    \n",
    "    tensor_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "    libcudnn.cudnnSetTensor4dDescriptor(tensor_desc, tensor_format, data_type, n, c, h, w)\n",
    "    return tensor_desc\n",
    "\n",
    "def create_pooling_2d_desc(mode, propogation_mode, windowHeight, windowWidth, verticalPadding, \n",
    "                           horizontalPadding, verticalStride, horizontalStride):\n",
    "    pooling_desc = libcudnn.cudnnCreatePoolingDescriptor()\n",
    "    libcudnn.cudnnSetPooling2dDescriptor(pooling_desc, mode, propogation_mode,\n",
    "                                         windowHeight, windowWidth, \n",
    "                                         verticalPadding, horizontalPadding, \n",
    "                                         verticalStride, horizontalStride)\n",
    "    return pooling_desc\n",
    "\n",
    "def create_activation_desc(mode, reluNanOpt, coef):\n",
    "    \n",
    "    activation_desc = libcudnn.cudnnCreateActivationDescriptor()\n",
    "    libcudnn.cudnnSetActivationDescriptor(activation_desc, mode, reluNanOpt, coef)\n",
    "    return activation_desc\n",
    "\n",
    "def create_convolution_2d_desc(pad_h, pad_w, u, v, dilation_h, dilation_w, mode, computeType):\n",
    "    \n",
    "    conv2d_desc = libcudnn.cudnnCreateConvolutionDescriptor()\n",
    "    libcudnn.cudnnSetConvolution2dDescriptor(conv2d_desc, pad_h, pad_w, u, v, \n",
    "                                             dilation_h, dilation_w, mode, computeType)\n",
    "    return conv2d_desc\n",
    "\n",
    "def create_filter_4d_desc(data_type, tensor_format, k, c, h, w):\n",
    "    \n",
    "    filter4d_desc = libcudnn.cudnnCreateFilterDescriptor()\n",
    "    libcudnn.cudnnSetFilter4dDescriptor(filter4d_desc, data_type, tensor_format, k, c, h, w)\n",
    "    return filter4d_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_fwd_convolution_tensors(cudnn_context, tensor_format, data_type, batch_size, \n",
    "                                     conv_in_channels, conv_in_height, conv_in_width, \n",
    "                                     conv_out_channels, conv_kernel_size, convolution_mode,\n",
    "                                     convolution_algo):\n",
    "    \n",
    "    data_tensor_desc = create_4d_tensor_desc(tensor_format, data_type,\n",
    "                                        batch_size, conv_in_channels, conv_in_height, conv_in_width)\n",
    "    conv1_filter_desc = create_filter_4d_desc(data_type, tensor_format,\n",
    "                                         conv_out_channels, conv_in_channels, \n",
    "                                         conv_kernel_size, conv_kernel_size)\n",
    "    conv1_desc = create_convolution_2d_desc(0, 0, 1, 1, 1, 1, convolution_mode, data_type)\n",
    "    [n, c, h, w] = libcudnn.cudnnGetConvolution2dForwardOutputDim(conv1_desc, \n",
    "                                                                  data_tensor_desc, \n",
    "                                                                  conv1_filter_desc)\n",
    "    \n",
    "    conv1_tensor_desc = create_4d_tensor_desc(tensor_format, data_type, n, c, h, w)\n",
    "    ws_conv1_size = libcudnn.cudnnGetConvolutionForwardWorkspaceSize(cudnn_context,\n",
    "                                                                     data_tensor_desc,\n",
    "                                                                     conv1_filter_desc,\n",
    "                                                                     conv1_desc,\n",
    "                                                                     conv1_tensor_desc,\n",
    "                                                                     convolution_algo)\n",
    "    \n",
    "    return [data_tensor_desc, conv1_filter_desc, conv1_desc, conv1_tensor_desc, ws_conv1_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_lenet:\n",
    "    \n",
    "    def __init__(self, cudnn_context, batch_size, width, height):\n",
    "        \n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.tensor_format = libcudnn.cudnnTensorFormat['CUDNN_TENSOR_NCHW']\n",
    "        self.data_type = libcudnn.cudnnDataType['CUDNN_DATA_FLOAT']\n",
    "        self.data_type_np = np.float32\n",
    "        self.pooling_format = libcudnn.cudnnPoolingMode['CUDNN_POOLING_MAX']\n",
    "        self.activation_mode = libcudnn.cudnnActivationMode['CUDNN_ACTIVATION_RELU']\n",
    "        self.propogation_mode = libcudnn.cudnnNanPropagation['CUDNN_PROPAGATE_NAN']\n",
    "        self.convolution_mode = libcudnn.cudnnConvolutionMode['CUDNN_CROSS_CORRELATION']\n",
    "        # !!!! The convolution is found by function calls in the LeNet code. Is choosing this ok? !!!!\n",
    "        self.convolution_algo = libcudnn.cudnnConvolutionFwdAlgo['CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM']\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Some constants !!!! Can these be removed? Maybe just hardcoded. !!!!\n",
    "        self.pool1_size = 2\n",
    "        self.pool1_stride = 2\n",
    "        self.pool2_size = 2\n",
    "        self.pool2_stride = 2\n",
    "        self.fc1_outputs = 500\n",
    "        self.fc2_outputs = 10\n",
    "        \n",
    "        self.conv1_in_channels = 1\n",
    "        self.conv1_out_channels = 20\n",
    "        self.conv1_kernel_size = 5\n",
    "        self.conv1_in_width = width\n",
    "        self.conv1_in_height = height\n",
    "        self.conv1_out_width = self.conv1_in_width - self.conv1_kernel_size + 1\n",
    "        self.conv1_out_height = self.conv1_in_height - self.conv1_kernel_size + 1\n",
    "        \n",
    "        self.conv2_in_channels = self.conv1_out_channels\n",
    "        self.conv2_out_channels = 50\n",
    "        self.conv2_kernel_size = 5\n",
    "        self.conv2_in_width = self.conv1_out_width // self.pool1_stride\n",
    "        self.conv2_in_height = self.conv1_out_height // self.pool1_stride\n",
    "        self.conv2_out_width = self.conv2_in_width - self.conv2_kernel_size + 1\n",
    "        self.conv2_out_height = self.conv2_in_height - self.conv2_kernel_size + 1\n",
    "\n",
    "        # Reserve memory\n",
    "        w_conv1 = np.zeros((self.conv1_in_channels, \n",
    "                            self.conv1_kernel_size, \n",
    "                            self.conv1_kernel_size, \n",
    "                            self.conv1_out_channels), dtype=self.data_type_np)\n",
    "        self.w_conv1_gpu = gpuarray.to_gpu(w_conv1)\n",
    "        \n",
    "        self.conv1_output_gpu = gpuarray.empty((self.batch_size,\n",
    "                                                self.conv1_out_channels,\n",
    "                                                self.conv1_out_height,\n",
    "                                                self.conv1_out_width), self.data_type_np)\n",
    "        \n",
    "        w_conv1_bias_tensor = np.zeros((self.conv1_out_channels, ), dtype=self.data_type_np)\n",
    "        self.conv1_bias_tensor_gpu = gpuarray.to_gpu(w_conv1_bias_tensor)\n",
    "        \n",
    "        # Define tensor descriptors\n",
    "        self.conv1_bias_tensor_desc = create_4d_tensor_desc(self.tensor_format, self.data_type, \n",
    "                                                       1, self.conv1_out_channels, 1, 1)\n",
    "        self.conv2_bias_tensor_desc = create_4d_tensor_desc(self.tensor_format, self.data_type, \n",
    "                                                       1, self.conv2_out_channels, 1, 1)\n",
    "        self.pooling_desc = create_pooling_2d_desc(self.pooling_format, self.propogation_mode,\n",
    "                                                   self.pool1_size, self.pool1_size,\n",
    "                                                   0, 0,\n",
    "                                                   self.pool1_stride, self.pool1_stride)\n",
    "        self.pooling2_tensor_desc = create_4d_tensor_desc(self.tensor_format,\n",
    "                                                     self.data_type,\n",
    "                                                     self.batch_size, self.conv2_out_channels,\n",
    "                                                     self.conv2_out_height // self.pool2_stride,\n",
    "                                                     self.conv2_out_width // self.pool2_stride)\n",
    "        self.fc1_tensor_desc = create_4d_tensor_desc(self.tensor_format, self.data_type,\n",
    "                                                self.batch_size, self.fc1_outputs, 1, 1)\n",
    "        self.fc2_tensor_desc = create_4d_tensor_desc(self.tensor_format, self.data_type,\n",
    "                                                self.batch_size, self.fc2_outputs, 1, 1)\n",
    "        self.fc1_activation_desc = create_activation_desc(self.activation_mode,\n",
    "                                                          self.propogation_mode, 0.0)\n",
    "\n",
    "        [self.data_tensor_desc, \n",
    "         self.conv1_filter_desc, \n",
    "         self.conv1_desc, \n",
    "         self.conv1_tensor_desc, \n",
    "         ws_conv1_size] = allocate_fwd_convolution_tensors(self.cudnn_context, self.tensor_format, self.data_type, \n",
    "                                         self.batch_size, self.conv1_in_channels, self.conv1_in_height,\n",
    "                                         self.conv1_in_width, self.conv1_out_channels,\n",
    "                                         self.conv1_kernel_size, self.convolution_mode,\n",
    "                                         self.convolution_algo)\n",
    "\n",
    "        [self.pooling1_tensor_desc, \n",
    "         self.conv2_filter_desc, \n",
    "         self.conv2_desc, \n",
    "         self.conv2_tensor_desc, \n",
    "         ws_conv2_size] = allocate_fwd_convolution_tensors(self.cudnn_context, self.tensor_format, self.data_type, \n",
    "                                         self.batch_size, self.conv2_in_channels, self.conv2_in_height,\n",
    "                                         self.conv2_in_width, self.conv2_out_channels,\n",
    "                                         self.conv2_kernel_size, self.convolution_mode,\n",
    "                                         self.convolution_algo)\n",
    "\n",
    "        self.workspace_size = max([ws_conv1_size, ws_conv2_size])\n",
    "        if(self.workspace_size > 0):\n",
    "            print(f\"Warning: workspace is not 0: {self.workspace_size}\")\n",
    "        self.workspace = None\n",
    "        \n",
    "    \n",
    "    def forward(self, data_desc, data_gpu):\n",
    "        \n",
    "        \n",
    "        alpha = 1.0\n",
    "        beta = 0.0\n",
    "\n",
    "        # Conv1 layer\n",
    "        libcudnn.cudnnConvolutionForward(self.cudnn_context, \n",
    "                                         alpha,\n",
    "                                         data_desc, data_gpu.ptr, \n",
    "                                         self.conv1_filter_desc, self.w_conv1_gpu.ptr,\n",
    "                                         self.conv1_desc, \n",
    "                                         self.convolution_algo,\n",
    "                                         self.workspace, \n",
    "                                         self.workspace_size,\n",
    "                                         beta,\n",
    "                                         self.conv1_tensor_desc,\n",
    "                                         self.conv1_output_gpu.ptr)\n",
    "        libcudnn.cudnnAddTensor(self.cudnn_context, \n",
    "                                alpha, \n",
    "                                self.conv1_bias_tensor_desc,\n",
    "                                self.conv1_bias_tensor_gpu.ptr,\n",
    "                                alpha,\n",
    "                                self.conv1_tensor_desc,\n",
    "                                self.conv1_output_gpu.ptr)\n",
    "        \n",
    "        '''\n",
    "        // Pool1 layer\n",
    "        checkCUDNN(cudnnPoolingForward(cudnnHandle, poolDesc, &alpha, conv1Tensor,\n",
    "                                       conv1, &beta, pool1Tensor, pool1));\n",
    "\n",
    "        // Conv2 layer\n",
    "        checkCUDNN(cudnnConvolutionForward(cudnnHandle, &alpha, pool1Tensor,\n",
    "                                           pool1, conv2filterDesc, pconv2, conv2Desc, \n",
    "                                           conv2algo, workspace, m_workspaceSize, &beta,\n",
    "                                           conv2Tensor, conv2));\n",
    "        checkCUDNN(cudnnAddTensor(cudnnHandle, &alpha, conv2BiasTensor,\n",
    "                                  pconv2bias, &alpha, conv2Tensor, conv2));\n",
    "\n",
    "        // Pool2 layer\n",
    "        checkCUDNN(cudnnPoolingForward(cudnnHandle, poolDesc, &alpha, conv2Tensor,\n",
    "                                       conv2, &beta, pool2Tensor, pool2));\n",
    "\n",
    "        // FC1 layer\n",
    "        // Forward propagate neurons using weights (fc1 = pfc1'*pool2)\n",
    "        checkCudaErrors(cublasSgemm(cublasHandle, CUBLAS_OP_T, CUBLAS_OP_N,\n",
    "                                    ref_fc1.outputs, m_batchSize, ref_fc1.inputs,\n",
    "                                    &alpha,\n",
    "                                    pfc1, ref_fc1.inputs,\n",
    "                                    pool2, ref_fc1.inputs,\n",
    "                                    &beta,\n",
    "                                    fc1, ref_fc1.outputs));\n",
    "        // Add bias using GEMM's \"beta\" (fc1 += pfc1bias*1_vec')\n",
    "        checkCudaErrors(cublasSgemm(cublasHandle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                                    ref_fc1.outputs, m_batchSize, 1,\n",
    "                                    &alpha,\n",
    "                                    pfc1bias, ref_fc1.outputs,\n",
    "                                    onevec, 1,\n",
    "                                    &alpha,\n",
    "                                    fc1, ref_fc1.outputs));\n",
    "\n",
    "        // ReLU activation\n",
    "        checkCUDNN(cudnnActivationForward(cudnnHandle, fc1Activation, &alpha,\n",
    "                                          fc1Tensor, fc1, &beta, fc1Tensor, fc1relu));\n",
    "\n",
    "        // FC2 layer\n",
    "        // Forward propagate neurons using weights (fc2 = pfc2'*fc1relu)\n",
    "        checkCudaErrors(cublasSgemm(cublasHandle, CUBLAS_OP_T, CUBLAS_OP_N,\n",
    "                                    ref_fc2.outputs, m_batchSize, ref_fc2.inputs,\n",
    "                                    &alpha,\n",
    "                                    pfc2, ref_fc2.inputs,\n",
    "                                    fc1relu, ref_fc2.inputs,\n",
    "                                    &beta,\n",
    "                                    fc2, ref_fc2.outputs));\n",
    "        // Add bias using GEMM's \"beta\" (fc2 += pfc2bias*1_vec')\n",
    "        checkCudaErrors(cublasSgemm(cublasHandle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
    "                                    ref_fc2.outputs, m_batchSize, 1,\n",
    "                                    &alpha,\n",
    "                                    pfc2bias, ref_fc2.outputs,\n",
    "                                    onevec, 1,\n",
    "                                    &alpha,\n",
    "                                    fc2, ref_fc2.outputs));\n",
    "\n",
    "        // Softmax loss\n",
    "        checkCUDNN(cudnnSoftmaxForward(cudnnHandle, CUDNN_SOFTMAX_ACCURATE, CUDNN_SOFTMAX_MODE_CHANNEL,\n",
    "                                       &alpha, fc2Tensor, fc2, &beta, fc2Tensor, result));\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_lenet(cudnn_context, batch_size=1, width=28, height=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data = np.zeros((1, 1, 28, 28), dtype=np.float32)\n",
    "inf_data_gpu = gpuarray.to_gpu(inf_data)\n",
    "inf_data_desc = create_4d_tensor_desc(model.tensor_format, model.data_type, 1, 1, 28, 28)\n",
    "model.forward(inf_data_desc, inf_data_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.conv1_output_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
