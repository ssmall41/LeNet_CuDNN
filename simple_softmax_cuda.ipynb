{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "import libcudnn, ctypes\n",
    "import pycuda.driver as drv\n",
    "import pandas as pd\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some globals. Should be defined in a constructor for a sequential class.\n",
    "\n",
    "# Initialize the cuDNN context\n",
    "cudnn_context = libcudnn.cudnnCreate()\n",
    "\n",
    "# Set some options and tensor dimensions\n",
    "tensor_format = libcudnn.cudnnTensorFormat['CUDNN_TENSOR_NCHW']\n",
    "data_type = libcudnn.cudnnDataType['CUDNN_DATA_FLOAT']\n",
    "data_type_np = np.float32\n",
    "softmax_mode = libcudnn.cudnnSoftmaxMode['CUDNN_SOFTMAX_MODE_INSTANCE']\n",
    "softmax_algo = libcudnn.cudnnSoftmaxAlgorithm['CUDNN_SOFTMAX_ACCURATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions for analysis and testing\n",
    "def analyze_prediction(preds, actuals):\n",
    "    \n",
    "    n_data, _ = preds.shape\n",
    "    preds_sparse = np.argmax(preds, axis=1)\n",
    "    actuals_sparse = np.argmax(actuals, axis=1)\n",
    "    return sum(preds_sparse == actuals_sparse) / n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_data4():\n",
    "    \n",
    "    n_features = 4\n",
    "    n_classes = 2\n",
    "    n_data = 10\n",
    "    \n",
    "    X_train = np.random.rand(n_data, n_features)\n",
    "    idx = np.array([1 if X_train[i][0] > 0.5 else 0 for i in range(len(X_train))])\n",
    "    Y_train = np.zeros((n_data, n_classes))\n",
    "    for i, row in enumerate(Y_train):\n",
    "        Y_train[i][idx[i]] = 1\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "def simple_data_fixed():\n",
    "    \n",
    "    X_train = np.array([[0.16135288, 0.30464995, 0.22296312, 0.2271508 ],\n",
    "       [0.44617278, 0.25550829, 0.31906067, 0.53594914],\n",
    "       [0.55487757, 0.36622737, 0.60981441, 0.55972613],\n",
    "       [0.99677183, 0.75638836, 0.06633571, 0.60571115],\n",
    "       [0.35337454, 0.34625126, 0.7179572 , 0.80923418],\n",
    "       [0.62548399, 0.69020976, 0.97985429, 0.16891352],\n",
    "       [0.59629351, 0.5056652 , 0.50843348, 0.7934896 ],\n",
    "       [0.46224334, 0.70235517, 0.26416074, 0.97176787],\n",
    "       [0.33548574, 0.64162364, 0.37395584, 0.32417411],\n",
    "       [0.21933534, 0.96979702, 0.41459635, 0.57283444]], dtype=np.float32)\n",
    "    \n",
    "    Y_train = np.array([[1., 0.],\n",
    "       [1., 0.],\n",
    "       [0., 1.],\n",
    "       [0., 1.],\n",
    "       [1., 0.],\n",
    "       [0., 1.],\n",
    "       [0., 1.],\n",
    "       [1., 0.],\n",
    "       [1., 0.],\n",
    "       [1., 0.]], dtype=np.float32)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def iris_data():\n",
    "    \n",
    "    df = pd.read_csv(\"data/iris.csv\") #.sample(10)\n",
    "    Y_train = pd.get_dummies(df[\"species\"]).to_numpy()\n",
    "    X_train = df.drop(columns=[\"species\"]).to_numpy()\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "#train_data_np, train_labels_np = simple_data_fixed()\n",
    "train_data_np, train_labels_np = iris_data()\n",
    "n_train, n_features = train_data_np.shape\n",
    "_, n_classes = train_labels_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_nn:\n",
    "    def __init__(self, cudnn_context, X_train, X_desc, Y_train, \n",
    "                 Y_desc, iterations, learning_rate, n_features, n_layers, n_classes,\n",
    "                 data_type_np, tensor_format, data_type):\n",
    "        \n",
    "        if n_layers > 1:\n",
    "            print(\"Support for greater than 1 layer not yet supported.\")\n",
    "            return\n",
    "        \n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "        self.sequential_layers = [None]*n_layers\n",
    "        self.layer_sizes = [0]*(n_layers+1)\n",
    "        self.layer_sizes[0] = n_features\n",
    "        \n",
    "        # Define loss as cross entropy\n",
    "        self.loss = cross_entropy_loss(cudnn_context, n_classes, \n",
    "                                       n_train, data_type_np, tensor_format, data_type)\n",
    "        \n",
    "        # ToDo: allocate more layers\n",
    "        \n",
    "        # Set the output softmax layer\n",
    "        self.layer_sizes[-1] = n_classes\n",
    "        self.sequential_layers[-1] = softmax(cudnn_context, \n",
    "                                             n_classes, \n",
    "                                             n_train, \n",
    "                                             n_features, \n",
    "                                             data_type_np, \n",
    "                                             tensor_format, \n",
    "                                             data_type)\n",
    "        \n",
    "        # Fit the model parameters to the data\n",
    "        self._fit(X_train, X_desc, Y_train, Y_desc, iterations, learning_rate)  # Pass in only n_train and do this later?\n",
    "    \n",
    "    def _fit(self, X_train, X_train_desc, Y_train, Y_train_desc, iterations, learning_rate):\n",
    "\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            _, _ = self.forward(X_train, X_train_desc)\n",
    "            self.backward(X_train, X_train_desc, Y_train, Y_train_desc)\n",
    "            self._apply_updates(learning_rate)\n",
    "            \n",
    "    def forward(self, X, X_desc):\n",
    "        \n",
    "        X_in = X\n",
    "        X_in_desc = X_desc\n",
    "        for layer in range(self.n_layers):\n",
    "            current_layer = self.sequential_layers[layer]\n",
    "            X_in, X_in_desc = current_layer.forward(X_in, X_in_desc)\n",
    "        return X_in, X_in_desc\n",
    "    \n",
    "    def backward(self, X_train, X_train_desc, Y_train, Y_train_desc):\n",
    "        \n",
    "        last_layer = self.sequential_layers[-1]\n",
    "        preds = last_layer.Y\n",
    "        dY, dY_desc = self.loss.derivative(Y_train, preds)\n",
    "        \n",
    "        for layer in range(self.n_layers-1, -1, -1):\n",
    "            current_layer = self.sequential_layers[layer]\n",
    "            \n",
    "            if layer > 0:\n",
    "                previous_layer = self.sequential_layers[layer-1]\n",
    "                X = previous_layer.Y\n",
    "                X_desc = previous_layer.Y_desc\n",
    "            else:\n",
    "                X = X_train\n",
    "                X_desc = X_train_desc\n",
    "            \n",
    "            dY, dY_desc = current_layer.backward(dY, dY_desc, X, X_desc)\n",
    "    \n",
    "    def _apply_updates(self, learning_rate):\n",
    "    \n",
    "        for layer in range(self.n_layers):\n",
    "            \n",
    "            current_layer = self.sequential_layers[layer]\n",
    "            current_layer.apply_updates(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, cudnn_context, n_train, data_type_np, tensor_format, data_type):\n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.n_train = n_train\n",
    "        self.data_type_np = data_type_np\n",
    "        self.tensor_format = tensor_format\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def forward(self, X, X_desc):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dY, dY_desc, X, X_desc):\n",
    "        pass\n",
    "\n",
    "\n",
    "class softmax(layer):\n",
    "    def __init__(self, cudnn_context, n_classes, n_train, n_input, data_type_np, tensor_format, data_type):\n",
    "        \n",
    "        super().__init__(cudnn_context, n_train, data_type_np, tensor_format, data_type)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_input = n_input\n",
    "        \n",
    "        #W_np = np.ones((n_classes, n_input))  # !!!! Make sure to initialize better !!!!\n",
    "        #b_np = np.ones((n_classes, 1))\n",
    "        W_np = np.random.normal(size=(n_classes, n_input), scale=0.1)\n",
    "        b_np = np.random.normal(size=(n_classes, 1), scale=0.5)\n",
    "        self.W = gpuarray.to_gpu(W_np)\n",
    "        self.b = gpuarray.to_gpu(b_np)\n",
    "        self.W_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.W_desc, tensor_format, data_type, \n",
    "                                            n_classes, n_input, 1, 1)\n",
    "        self.b_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.b_desc, tensor_format, data_type, \n",
    "                                            n_classes, 1, 1, 1)\n",
    "        \n",
    "        # Output\n",
    "        self.Y = gpuarray.empty((n_train, n_classes, 1, 1), data_type_np)\n",
    "        self.Y_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.Y_desc, tensor_format, data_type, \n",
    "                                            n_train, n_classes, 1, 1)\n",
    "        self.dX = None  # Need to implement!\n",
    "        self.dX_desc = None\n",
    "        \n",
    "        # Updates\n",
    "        self.weight_updates = gpuarray.empty((n_classes, n_input+1, 1, 1), data_type_np)\n",
    "        self.weight_updates_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.weight_updates_desc, tensor_format, data_type, \n",
    "                                            n_classes, n_input+1, 1, 1)\n",
    "        \n",
    "        # Working space\n",
    "        self.z = gpuarray.empty((n_train, n_classes, 1, 1), data_type_np)\n",
    "        self.z_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.z_desc, tensor_format, data_type, \n",
    "                                            n_train, n_classes, 1, 1)\n",
    "        self.softmax_grad = gpuarray.empty((n_train, n_classes, 1, 1), data_type_np)\n",
    "        self.softmax_grad_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.softmax_grad_desc, tensor_format, data_type, \n",
    "                                            n_train, n_classes, 1, 1)\n",
    "\n",
    "    def forward(self, X, X_desc):\n",
    "        \n",
    "        # Note: X = layer_output, Y = output_pred\n",
    "\n",
    "        # ToDo: Get rid of numpy usage\n",
    "        # Apply the linear combination of weights and inputs\n",
    "        W_np = self.W.get().reshape((self.n_classes, self.n_input))\n",
    "        b_np = self.b.get().reshape((self.n_classes, 1))\n",
    "        X_np = X.get()\n",
    "\n",
    "        # The matrix here is interpreted as the transpose\n",
    "        z_np = (np.matmul(W_np, X_np.T) + b_np).T.reshape((self.n_classes, n_train, 1, 1))\n",
    "        z = gpuarray.to_gpu(z_np.astype(data_type_np))\n",
    "        self.z = z  #Note: This is only done because of numpy\n",
    "\n",
    "        # Apply the forward softmax\n",
    "        libcudnn.cudnnSoftmaxForward(handle=self.cudnn_context,\n",
    "                                     algorithm=softmax_algo, \n",
    "                                     mode=softmax_mode, \n",
    "                                     alpha=1.0,\n",
    "                                     srcDesc=self.z_desc,\n",
    "                                     srcData=self.z.ptr,\n",
    "                                     beta=0.0,\n",
    "                                     destDesc=self.Y_desc,\n",
    "                                     destData=self.Y.ptr)\n",
    "        return self.Y, self.Y_desc\n",
    "    \n",
    "    def backward(self, dY, dY_desc, X, X_desc):\n",
    "        \n",
    "        # Note: Y = output_pred, dY = d_entropy, X = prev_layer_output, dX = ????\n",
    "        \n",
    "        # Calculate the backward softmax\n",
    "        libcudnn.cudnnSoftmaxBackward(handle=self.cudnn_context,\n",
    "                                      algorithm=softmax_algo,\n",
    "                                      mode=softmax_mode,\n",
    "                                      alpha=1.0,\n",
    "                                      srcDesc=self.Y_desc,\n",
    "                                      srcData=self.Y.ptr,\n",
    "                                      srcDiffDesc=dY_desc,\n",
    "                                      srcDiffData=dY.ptr,\n",
    "                                      beta=0.0,\n",
    "                                      destDiffDesc=self.softmax_grad_desc,\n",
    "                                      destDiffData=self.softmax_grad.ptr)\n",
    "\n",
    "        # Calculate the gradient updates\n",
    "        # ToDo: remove numpy usage\n",
    "        softmax_grad_np = self.softmax_grad.get().reshape((self.n_train, self.n_classes))\n",
    "        weight_updates_np = np.zeros((self.n_classes, self.n_input+1))\n",
    "        X_aug_np = np.concatenate([X.get(), np.ones((self.n_train, 1))], axis=1)\n",
    "        for i in range(self.n_classes):\n",
    "            weight_updates_np[i, :] = np.mean(X_aug_np * softmax_grad_np[:, i][:, np.newaxis], axis=0)\n",
    "\n",
    "        weight_updates = gpuarray.to_gpu(weight_updates_np.reshape(self.n_classes, self.n_input+1, 1, 1).astype(self.data_type_np))\n",
    "        self.weight_updates = weight_updates  # Note: only needed for numpy\n",
    "        \n",
    "        return self.dX, self.dX_desc\n",
    "\n",
    "    def apply_updates(self, learning_rate):\n",
    "\n",
    "        # ToDo: remove numpy usage\n",
    "        weight_updates_np = self.weight_updates.get().reshape(self.n_classes, self.n_input+1)\n",
    "        W_np = self.W.get().reshape(self.n_classes, self.n_input)\n",
    "        b_np = self.b.get().reshape(self.n_classes, 1)\n",
    "\n",
    "        W_updates_np = weight_updates_np[:, 0:self.n_input]\n",
    "        b_updates_np = weight_updates_np[:, -1].reshape(self.n_classes, 1)\n",
    "\n",
    "        W_np_new_np = W_np - learning_rate*W_updates_np\n",
    "        b_np_new_np = b_np - learning_rate*b_updates_np\n",
    "\n",
    "        self.W = gpuarray.to_gpu(W_np_new_np.reshape(self.n_classes, self.n_input, 1, 1).astype(self.data_type_np))\n",
    "        self.b = gpuarray.to_gpu(b_np_new_np.reshape(self.n_classes, 1, 1, 1).astype(self.data_type_np))\n",
    "\n",
    "\n",
    "class cross_entropy_loss:\n",
    "    def __init__(self, cudnn_context, n_classes, n_train, data_type_np, tensor_format, data_type):\n",
    "        self.cudnn_context = cudnn_context\n",
    "        self.data_type_np = data_type_np\n",
    "        self.tensor_format = tensor_format\n",
    "        self.data_type = data_type\n",
    "        self.n_classes = n_classes\n",
    "        self.n_train = n_train\n",
    "        \n",
    "        # Define CUDA related data\n",
    "        module = SourceModule(open(\"kernels.cu\", \"r\").read())\n",
    "        self.kernel_deriv_entropy = module.get_function(\"deriv_entropy\")\n",
    "        self.n_classes_c = np.int32(n_classes)\n",
    "        self.n_train_c = np.int32(n_train)\n",
    "        self.block_size = 256\n",
    "        \n",
    "        self.dX = gpuarray.empty((n_train, n_classes, 1, 1), data_type_np)\n",
    "        self.dX_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "        libcudnn.cudnnSetTensor4dDescriptor(self.dX_desc, tensor_format, data_type, \n",
    "                                            n_train, n_classes, 1, 1)\n",
    "        \n",
    "    def calc_loss(self, X, X_desc):\n",
    "        # Note: should have X = output from previous layer, Y = output of this layer\n",
    "        \n",
    "        #Y_np = X.get()  # Can we copy from device to device?\n",
    "        #self.Y = gpuarray.to_gpu(Y_np)  # Just because of numpy\n",
    "        return None\n",
    "    \n",
    "    def derivative(self, Y_train, preds):\n",
    "        # Note: should have Y = Y_train, preds = output_pred, dX = d_entropy\n",
    "        \n",
    "        self.kernel_deriv_entropy(self.n_train_c,\n",
    "                                  self.n_classes_c,\n",
    "                                  Y_train,\n",
    "                                  preds,\n",
    "                                  self.dX,\n",
    "                                  block=(self.n_classes, 1, 1),\n",
    "                                  grid=(self.block_size, 1, 1))\n",
    "        \n",
    "        return self.dX, self.dX_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Format input data\n",
    "X_train = gpuarray.to_gpu(train_data_np.astype(data_type_np))\n",
    "X_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "libcudnn.cudnnSetTensor4dDescriptor(X_desc, tensor_format, data_type, \n",
    "                                    n_train, n_features, 1, 1)\n",
    "Y_train = gpuarray.to_gpu(train_labels_np.astype(data_type_np))\n",
    "Y_desc = libcudnn.cudnnCreateTensorDescriptor()\n",
    "libcudnn.cudnnSetTensor4dDescriptor(Y_desc, tensor_format, data_type, \n",
    "                                    n_train, n_classes, 1, 1)\n",
    "\n",
    "\n",
    "model = sequential_nn(cudnn_context, X_train, X_desc, Y_train, \n",
    "                      Y_desc, 500, 0.01, n_features, 1, n_classes,\n",
    "                      data_type_np, tensor_format, data_type)\n",
    "preds, preds_desc = model.forward(X_train, X_desc)\n",
    "\n",
    "analyze_prediction(preds.get().reshape((n_train, n_classes)), \n",
    "                   Y_train.get().reshape((n_train, n_classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.get().reshape((n_train, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.get().reshape((n_train, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.sequential_layers[0].weight_updates.get().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sequential_layers[0].Y.get().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sequential_layers[0].W.get().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sequential_layers[0].b.get().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
